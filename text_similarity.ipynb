{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set()\n",
    "with open('D:\\\\NLP\\\\case_recommender\\\\data\\\\stopwords.txt', \"r\", encoding=\"utf-8\") as f_stopwords:\n",
    "    for line in f_stopwords:\n",
    "        stopwords.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000,\n",
    "                             min_df=1, stop_words=stopwords,\n",
    "                             use_idf=True,lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\NLP\\\\case_recommender\\\\recommender.txt\", 'a', encoding='utf-8')\n",
    "basic_df = pd.read_csv('D:\\\\NLP\\\\case_recommender\\\\data\\\\basic_bj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('\\s+')\n",
    "raw_text_dataset = list()\n",
    "with open('D:\\\\NLP\\\\case_recommender\\\\data\\\\train1.txt', 'r', encoding='utf-8') as train_file:\n",
    "    lines=train_file.readlines()\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            continue\n",
    "        dataline = [x for x in re.sub(pattern, '', line) if x not in stopwords]\n",
    "        if dataline==[]:\n",
    "#             print(line)\n",
    "            continue\n",
    "        dataline=' '.join(dataline)\n",
    "        raw_text_dataset.append(dataline)\n",
    "# print(raw_text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('\\s+')\n",
    "raw_text_dataset = list()\n",
    "with open('D:\\\\NLP\\\\case_recommender\\\\data\\\\train1.txt', 'r', encoding='utf-8') as train_file:\n",
    "    raw_text_dataset=train_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yixuan\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 374)\t0.19856380323607276\n",
      "  (0, 5055)\t0.10807913964631191\n",
      "  (0, 5377)\t0.16366113949704003\n",
      "  (0, 3551)\t0.15291116463392274\n",
      "  (0, 9760)\t0.13589791015257863\n",
      "  (0, 8259)\t0.16627403754409345\n",
      "  (0, 3879)\t0.16915537409656903\n",
      "  (0, 2917)\t0.19268592248437008\n",
      "  (0, 8725)\t0.15702608772472842\n",
      "  (0, 7339)\t0.10226718581226832\n",
      "  (0, 6228)\t0.38056255222585933\n",
      "  (0, 1666)\t0.12635443697562596\n",
      "  (0, 5203)\t0.4142020884021312\n",
      "  (0, 7972)\t0.1423941237822024\n",
      "  (0, 5672)\t0.1655976581337749\n",
      "  (0, 3457)\t0.1253808693008583\n",
      "  (0, 5749)\t0.06379705875183651\n",
      "  (0, 5219)\t0.2121414459931781\n",
      "  (0, 6524)\t0.2396717413262426\n",
      "  (0, 3671)\t0.1790585010815732\n",
      "  (0, 7138)\t0.16840683940928003\n",
      "  (0, 370)\t0.12165280047476505\n",
      "  (0, 1935)\t0.12719236150688787\n",
      "  (0, 8928)\t0.23159696950198613\n",
      "  (0, 7652)\t0.11475439990006993\n",
      "  :\t:\n",
      "  (5637, 7616)\t0.11349625890358797\n",
      "  (5637, 1641)\t0.13092896347267874\n",
      "  (5637, 6145)\t0.13092896347267874\n",
      "  (5637, 9780)\t0.13092896347267874\n",
      "  (5637, 8661)\t0.13092896347267874\n",
      "  (5637, 2901)\t0.1163893248823228\n",
      "  (5637, 7761)\t0.13549388289930892\n",
      "  (5637, 9370)\t0.13549388289930892\n",
      "  (5637, 3217)\t0.13549388289930892\n",
      "  (5637, 2507)\t0.13549388289930892\n",
      "  (5637, 8215)\t0.13549388289930892\n",
      "  (5637, 8102)\t0.13549388289930892\n",
      "  (5637, 8139)\t0.13549388289930892\n",
      "  (5637, 7265)\t0.13549388289930892\n",
      "  (5637, 2740)\t0.13549388289930892\n",
      "  (5637, 3732)\t0.13549388289930892\n",
      "  (5637, 5242)\t0.13549388289930892\n",
      "  (5637, 2739)\t0.13549388289930892\n",
      "  (5637, 2508)\t0.13549388289930892\n",
      "  (5637, 8216)\t0.13549388289930892\n",
      "  (5637, 8136)\t0.13549388289930892\n",
      "  (5637, 8090)\t0.13549388289930892\n",
      "  (5637, 1422)\t0.13549388289930892\n",
      "  (5637, 2735)\t0.13549388289930892\n",
      "  (5637, 8750)\t0.13549388289930892\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(raw_text_dataset)\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = np.argsort(X_train_tfidf.toarray(), axis=1)[:, -10:-1]\n",
    "names = vectorizer.get_feature_names()\n",
    "keyWords = pd.Index(names)[sort].values\n",
    "# print(keyWords)\n",
    "# tagDF = pd.DataFrame({\n",
    "#     'ID':basic_df['ID'],\n",
    "#     'Illegal Facts': lines,\n",
    "#     'tag1': keyWords[:, 0],\n",
    "#     'tag2': keyWords[:, 1],\n",
    "#     'tag3': keyWords[:, 2],\n",
    "#     'tag4': keyWords[:, 3],\n",
    "#     'tag5': keyWords[:, 4],\n",
    "#     'tag6': keyWords[:, 5],\n",
    "#     'tag7': keyWords[:, 6],\n",
    "#     'tag8': keyWords[:, 7],\n",
    "#     'tag9': keyWords[:, 8],\n",
    "#     'tag10': keyWords[:, 9],\n",
    "# })\n",
    "# tagDF.to_csv(path_or_buf='keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(keyWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入要查询的词：武翼\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'keywords.csv' does not exist: b'keywords.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-476efe52cda0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m\"exit()\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"keywords.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Illegal Facts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtotal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'keywords.csv' does not exist: b'keywords.csv'"
     ]
    }
   ],
   "source": [
    "# 计算关键词召回率\n",
    "while True:\n",
    "    word=input(\"请输入要查询的词：\")\n",
    "    if word ==\"exit()\":\n",
    "        break\n",
    "    df=pd.read_csv(\"keywords.csv\")\n",
    "    bool = df['Illegal Facts'].str.contains(word)\n",
    "    total_data = df[bool]\n",
    "    total=len(total_data)\n",
    "    recall_data=0\n",
    "    for i in range(3,13):\n",
    "        bool=total_data.iloc[:,i].str.contains(word)\n",
    "        recall_data=len(total_data[bool])+recall_data\n",
    "    print(\"total: %d , recalled: %d ,召回率: %f\"%(total,recall_data,0 if total ==0 else recall_data/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual number of tfidf features: 10000\n",
      "\n",
      "Performing dimensionality reduction using LSA\n"
     ]
    }
   ],
   "source": [
    "print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[1])\n",
    "\n",
    "print(\"\\nPerforming dimensionality reduction using LSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Explained variance of the SVD step: 43%\n"
     ]
    }
   ],
   "source": [
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"X_train_lsa.txt\", X_train_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=list()\n",
    "case_id_list=list()\n",
    "case_order_list=list()\n",
    "with open('D:\\\\NLP\\\\case_recommender\\\\data\\\\test.txt','r',encoding='utf-8') as test_file:\n",
    "    for line in test_file.readlines():\n",
    "        case_id_list.append(line.split('\\t')[0])\n",
    "        case_order_list.append(basic_df[basic_df.ID == line.split('\\t')[0]].index.tolist()[0])\n",
    "        line=line.split('\\t')[2]\n",
    "        dataline = [x for x in re.sub(pattern, '', line) if x not in stopwords]\n",
    "        dataline=''.join(dataline)\n",
    "        x_test.append(dataline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        ID                NAME  \\\n",
      "1137  f4630316-8dec-4533-976c-614bd37ffb25  北京万寿宾馆不按照排水证要求排放污水   \n",
      "\n",
      "                                          ILLEGAL_FACTS           POWER_NAME  \n",
      "1137  经查实，北京万寿宾馆于2017年6月在北京市海淀区万寿路甲12号北京万寿宾馆北侧向排水管网排...  对向排水管网排放超标污水的行为进行处罚  \n"
     ]
    }
   ],
   "source": [
    "target = np.random.randint(0, high=len(x_test))\n",
    "c = case_id_list[target]\n",
    "target_line1 = basic_df[basic_df.ID == c]\n",
    "print(target_line1)\n",
    "targic_order=basic_df[basic_df.ID == c].index.tolist()[0]\n",
    "target_line=X_train_lsa[targic_order,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00285008,  0.09677043,  0.12346221, -0.0123612 , -0.00591373,\n",
       "        0.01474913,  0.3697168 , -0.11465778,  0.46822896,  0.0516633 ,\n",
       "        0.31796029,  0.40815074, -0.00370913, -0.00796897, -0.01694204,\n",
       "       -0.02379519, -0.16190328,  0.00664479,  0.07193776,  0.07224887,\n",
       "       -0.02125821,  0.05637719,  0.06661785, -0.00246098,  0.00497015,\n",
       "       -0.01834292,  0.04323406, -0.00832167, -0.00235983,  0.01613201,\n",
       "       -0.01513757,  0.03882555,  0.00655477, -0.04554621, -0.02136803,\n",
       "        0.01309819,  0.01152848, -0.01590617,  0.00781233,  0.01611648,\n",
       "       -0.02017178, -0.01531617, -0.01026382,  0.03393673,  0.02954936,\n",
       "        0.04891105,  0.04538919,  0.08825546, -0.05152374,  0.04087342,\n",
       "        0.14438659,  0.11146864, -0.07004775, -0.13414104, -0.06936653,\n",
       "        0.15670792,  0.02589441,  0.03558043,  0.05241349, -0.21211651,\n",
       "        0.03587547,  0.04899033,  0.13493556, -0.08818805, -0.03705774,\n",
       "        0.01389464,  0.04029228, -0.09603607, -0.03573088,  0.0719517 ,\n",
       "       -0.01328048,  0.02909226, -0.01738972,  0.04073645,  0.02798489,\n",
       "       -0.10103902, -0.00230277,  0.0197289 ,  0.01677239,  0.02062094,\n",
       "       -0.00560109, -0.01058301, -0.06438627,  0.01992952,  0.03821111,\n",
       "       -0.02507259,  0.00187076,  0.01130669, -0.04531843,  0.01587665,\n",
       "        0.01589687,  0.06858723,  0.02083629, -0.03614516,  0.02584274,\n",
       "        0.03211379, -0.09437772,  0.03314131,  0.13277431, -0.13682462])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targic_order=basic_df[basic_df.ID == c].index.tolist()[0]\n",
    "target_line=X_train_lsa[targic_order]\n",
    "# print(target_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_file=open ('recommender1.txt','a',encoding='utf-8')\n",
    "r_file.write(\"####################### 待推荐案例 ##########################\\n\")\n",
    "r_file.close()\n",
    "target_line1.to_csv('recommender1.txt', mode='a', header=False)\n",
    "r_file=open ('recommender1.txt','a',encoding='utf-8')\n",
    "r_file.write(\"####################### 推荐结果 ############################\\n\")\n",
    "r_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_tf_idf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6aa918d08121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# print(test_tf_idf.toarray())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_tf_idf.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_tf_idf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_tf_idf' is not defined"
     ]
    }
   ],
   "source": [
    "# print(test_tf_idf.toarray())\n",
    "np.savetxt(\"test_tf_idf.txt\",test_tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260360607785702\n",
      "0.8466574643344702\n",
      "0.8373043739765793\n",
      "0.8188832721555083\n",
      "0.8666431790922413\n",
      "0.8340822190293782\n",
      "0.8582766694090149\n",
      "0.8538707459613688\n",
      "0.8290697612466615\n",
      "0.8414457672749192\n",
      "0.8402306851582357\n",
      "0.8122618951193737\n",
      "0.825660217329731\n",
      "0.8310941761469026\n",
      "0.849882814022821\n",
      "0.8556580675022539\n",
      "0.8628116398623319\n",
      "0.8552693792662929\n",
      "0.8312039341198425\n",
      "0.857130393547848\n",
      "0.9276633720686188\n",
      "0.8356174591749841\n",
      "0.837759542086175\n",
      "0.8139694373319768\n",
      "0.832075410910545\n",
      "0.8263869699427646\n",
      "0.8343328893251031\n",
      "0.8328121645282784\n",
      "0.8597668400331637\n",
      "0.8316833957308202\n",
      "0.815787394739047\n",
      "0.945037272534131\n",
      "0.9118004417312411\n",
      "0.5795295776231444\n",
      "0.9713827187775773\n",
      "0.8150072064527313\n",
      "0.55375580995411\n",
      "0.8622054807155941\n",
      "0.9618006541712154\n",
      "0.9349330119662196\n",
      "1.0\n",
      "0.9793334767175186\n",
      "0.945050183928076\n",
      "0.725635503935479\n",
      "0.9381235846094802\n",
      "0.9665218841170273\n",
      "0.9339938994896482\n",
      "0.9788318654940152\n",
      "0.8837441063453579\n",
      "0.9215203356318642\n",
      "0.9415024366989264\n",
      "0.8319075492375916\n",
      "0.9706637227178923\n",
      "0.9367050516630051\n",
      "0.9468536414791722\n",
      "0.9337179312822657\n",
      "0.9474122682998533\n",
      "0.929801862752105\n",
      "0.840606098179453\n",
      "0.8677163531388252\n",
      "0.8781560536442341\n",
      "0.9323025063862863\n",
      "0.9599121757443974\n"
     ]
    }
   ],
   "source": [
    "recommander=dict()\n",
    "#开始推荐\n",
    "for case_order in case_order_list:\n",
    "    predicted_line=X_train_lsa[case_order,:]\n",
    "    vector_a = np.mat(target_line)\n",
    "    vector_b = np.mat(predicted_line)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    a=np.linalg.norm(vector_a)\n",
    "    b=np.linalg.norm(vector_b)\n",
    "    denom = a * b\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    recommander[case_order] = sim\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138, 0.9793334767175186)\n",
      "(1144, 0.9788318654940152)\n",
      "(1131, 0.9713827187775773)\n",
      "(1149, 0.9706637227178923)\n",
      "(1142, 0.9665218841170273)\n",
      "(1135, 0.9618006541712154)\n",
      "(1159, 0.9599121757443974)\n",
      "(1153, 0.9474122682998533)\n",
      "(1151, 0.9468536414791722)\n",
      "(1139, 0.945050183928076)\n"
     ]
    }
   ],
   "source": [
    "for pre in sorted(recommander.items(),key=lambda x:x[1],reverse=True)[1:11]:\n",
    "    basic_df.iloc[pre[0]].to_csv('recommender1.txt', mode='a', header=False)\n",
    "    print(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
